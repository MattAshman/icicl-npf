{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd226f-f56b-4c7b-b7c3-53527b071e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ce9594-2bc2-4d6f-809b-4632bb3f6b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from utils.data import DIR_DATA, GPDataset\n",
    "from utils.data.helpers import DatasetMerger\n",
    "\n",
    "from sklearn.gaussian_process.kernels import (\n",
    "    RBF,\n",
    "    ConstantKernel,\n",
    "    DotProduct,\n",
    "    ExpSineSquared,\n",
    "    Matern,\n",
    "    WhiteKernel,\n",
    ")\n",
    "\n",
    "def get_all_gp_datasets(**kwargs):\n",
    "    \"\"\"Return train / tets / valid sets for all GP experiments.\"\"\"\n",
    "    datasets, test_datasets, valid_datasets = dict(), dict(), dict()\n",
    "\n",
    "    for f in [\n",
    "        get_datasets_single_gp,\n",
    "        get_datasets_variable_hyp_gp,\n",
    "        get_datasets_variable_kernel_gp,\n",
    "    ]:\n",
    "        _datasets, _test_datasets, _valid_datasets = f(**kwargs)\n",
    "        datasets.update(_datasets)\n",
    "        test_datasets.update(_test_datasets)\n",
    "        valid_datasets.update(_valid_datasets)\n",
    "\n",
    "    return datasets, test_datasets, valid_datasets\n",
    "\n",
    "\n",
    "def get_datasets_single_gp(**kwargs):\n",
    "    \"\"\"Return train / tets / valid sets for 'Samples from a single GP'.\"\"\"\n",
    "    kernels = dict()\n",
    "\n",
    "    kernels[\"RBF_Kernel\"] = RBF(length_scale=(0.2))\n",
    "\n",
    "    kernels[\"Periodic_Kernel\"] = ExpSineSquared(length_scale=0.5, periodicity=0.5)\n",
    "\n",
    "    # kernels[\"Matern_Kernel\"] = Matern(length_scale=0.2, nu=1.5)\n",
    "\n",
    "    kernels[\"Noisy_Matern_Kernel\"] = WhiteKernel(noise_level=0.1) + Matern(\n",
    "        length_scale=0.2, nu=1.5\n",
    "    )\n",
    "\n",
    "    return get_gp_datasets(\n",
    "        kernels,\n",
    "        is_vary_kernel_hyp=False,  # use a single hyperparameter per kernel\n",
    "        n_samples=10_000,  # number of different context-target sets\n",
    "        n_points=128,  # size of target U context set for each sample\n",
    "        is_reuse_across_epochs=False,  # never see the same example twice\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_datasets_variable_hyp_gp(**kwargs):\n",
    "    \"\"\"Return train / tets / valid sets for 'Samples from GPs with varying Kernel hyperparameters'.\"\"\"\n",
    "    kernels = dict()\n",
    "\n",
    "    kernels[\"Variable_Matern_Kernel\"] = Matern(length_scale_bounds=(0.01, 0.3), nu=1.5)\n",
    "\n",
    "    return get_gp_datasets(\n",
    "        kernels,\n",
    "        is_vary_kernel_hyp=True,  # use a different hyp for each samples\n",
    "        n_samples=50000,  # number of different context-target sets\n",
    "        n_points=128,  # size of target U context set for each sample\n",
    "        is_reuse_across_epochs=False,  # never see the same example twice\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_datasets_variable_kernel_gp(**kwargs):\n",
    "    \"\"\"Return train / tets / valid sets for 'Samples from GPs with varying Kernels'.\"\"\"\n",
    "\n",
    "    datasets, test_datasets, valid_datasets = get_datasets_single_gp(**kwargs)\n",
    "    return (\n",
    "        dict(All_Kernels=DatasetMerger(datasets.values())),\n",
    "        dict(All_Kernels=DatasetMerger(test_datasets.values())),\n",
    "        dict(All_Kernels=DatasetMerger(valid_datasets.values())),\n",
    "    )\n",
    "\n",
    "\n",
    "def sample_gp_dataset_like(dataset, **kwargs):\n",
    "    \"\"\"Wrap the output of `get_samples` in a gp dataset.\"\"\"\n",
    "    new_dataset = copy.deepcopy(dataset)\n",
    "    new_dataset.set_samples_(*dataset.get_samples(**kwargs))\n",
    "    return new_dataset\n",
    "\n",
    "\n",
    "def get_gp_datasets(\n",
    "    kernels, save_file=f\"{os.path.join(DIR_DATA, 'gp_dataset.hdf5')}\", **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Return a train, test and validation set for all the given kernels (dict).\n",
    "    \"\"\"\n",
    "    datasets = dict()\n",
    "\n",
    "    def get_save_file(name, save_file=save_file):\n",
    "        if save_file is not None:\n",
    "            save_file = (save_file, name)\n",
    "        return save_file\n",
    "\n",
    "    for name, kernel in kernels.items():\n",
    "        datasets[name] = GPDataset(\n",
    "            kernel=kernel, save_file=get_save_file(name), **kwargs\n",
    "        )\n",
    "\n",
    "    datasets_test = {\n",
    "        k: sample_gp_dataset_like(\n",
    "            dataset, save_file=get_save_file(k), idx_chunk=-1, n_samples=10000\n",
    "        )\n",
    "        for k, dataset in datasets.items()\n",
    "    }\n",
    "\n",
    "    datasets_valid = {\n",
    "        k: sample_gp_dataset_like(\n",
    "            dataset,\n",
    "            save_file=get_save_file(k),\n",
    "            idx_chunk=-2,\n",
    "            n_samples=dataset.n_samples // 10,\n",
    "        )\n",
    "        for k, dataset in datasets.items()\n",
    "    }\n",
    "\n",
    "    return datasets, datasets_test, datasets_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7c4f4f-6de3-43cc-9bf6-ee09e9d95496",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_datasets, gp_test_datasets, gp_valid_datasets = get_datasets_single_gp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f98b724-a4a6-4164-81f1-4f05ff5b6964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from npf.utils.datasplit import CntxtTrgtGetter, GetRandomIndcs, get_all_indcs\n",
    "from utils.data import cntxt_trgt_collate\n",
    "\n",
    "get_cntxt_trgt_1d = cntxt_trgt_collate(\n",
    "    CntxtTrgtGetter(\n",
    "        contexts_getter=GetRandomIndcs(a=0.0, b=50), targets_getter=get_all_indcs,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db11a55f-3c8d-4f64-9831-aad23dc828ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_datasets[\"RBF_Kernel\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ac0ebc-91c5-4350-8c7f-c76c0e980f0a",
   "metadata": {},
   "source": [
    "# Build model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f683faff-baa6-44dd-b947-9c9c29387b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from npf import ICCNP, CNP\n",
    "from npf.architectures import MLP, merge_flat_input\n",
    "\n",
    "r_dim = 64\n",
    "ic_r_dim = 64\n",
    "\n",
    "model = ICCNP(x_dim=1, y_dim=1, r_dim=r_dim, ic_r_dim=ic_r_dim)\n",
    "# model = CNP(x_dim=1, y_dim=1, r_dim=r_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7dd092-2203-420a-8ebf-7fb091194dcd",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75040a5-0457-4345-b39f-172a90439c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ic_batch_generator(gp_datasets=gp_datasets, batch_size=16, n=128, max_n_cntxt=60, max_n_dc=2):\n",
    "    # Ranomly select kernel.\n",
    "    kernel_idx = np.random.randint(len(gp_datasets))\n",
    "    # kernel_idx = 0\n",
    "    kernel = list(gp_datasets.keys())[kernel_idx]\n",
    "    dataset_gen = gp_datasets[kernel]\n",
    "    \n",
    "    # We need (1 + n_dc) * batch_size datasets.\n",
    "    n_dc = np.random.randint(max_n_dc)\n",
    "    x, y = dataset_gen.get_samples(n_samples=(1 + n_dc) * batch_size, n_points=n)\n",
    "\n",
    "    # Keep first batch_size to split into target / context sets.\n",
    "    xt, yt = x[:batch_size], y[:batch_size]\n",
    "    n_cntxt = np.random.randint(max_n_cntxt)\n",
    "    idx_cntxt = torch.randperm(n)[:n_cntxt]\n",
    "    xc, yc = x[:batch_size, idx_cntxt], y[:batch_size, idx_cntxt]\n",
    "\n",
    "    # Create D_cntxt of shape (batch_size, n_dc, n, x_dim).\n",
    "    x_dc, y_dc = x[batch_size:], y[batch_size:] # (batch_size * n_dc, n, x_dim).\n",
    "    x_dc = x_dc.reshape(batch_size, n_dc, n, 1)\n",
    "    y_dc = y_dc.reshape(batch_size, n_dc, n, 1)\n",
    "    dc = (x_dc, y_dc)\n",
    "\n",
    "    return xc, yc, xt, yt, dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61da2d0e-05c9-4d2e-871c-5fbc6f09d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from npf import CNPFLoss\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 3\n",
    "lr = 1e-3\n",
    "decay_lr = 10\n",
    "max_epochs = 10\n",
    "iters = 50000 # batches per epoch.\n",
    "loss = CNPFLoss()\n",
    "\n",
    "gp_datasets, gp_test_datasets, gp_valid_datasets = get_datasets_single_gp()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "gamma = (1 / decay_lr) ** (1 / max_epochs)\n",
    "schedule = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=gamma)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(f\"Epoch: {epoch}.\")\n",
    "\n",
    "    tqdm_iter = tqdm(range(iters), desc=\"Iters\")\n",
    "    for _ in tqdm_iter:\n",
    "        #Â Generate batch of data.\n",
    "        xc, yc, xt, yt, dc = ic_batch_generator(gp_datasets, batch_size)\n",
    "\n",
    "        nll = loss.get_loss(*model(xc, yc, xt, yt, dc), yt)\n",
    "        # nll = loss.get_loss(*model(xc, yc, xt, yt), yt)}\n",
    "        nll = nll.mean()\n",
    "        opt.zero_grad()\n",
    "        nll.backward()\n",
    "        opt.step()\n",
    "\n",
    "        tqdm_iter.set_postfix({\"nll\": nll.item()})\n",
    "\n",
    "    # Reduce learning rate.\n",
    "    schedule.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd06b6b-aaa1-465e-8051-862592ee8aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wbml.plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualise_1d(mean, scale, xc, yc, xt, yt):\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Plot context and target.\n",
    "    plt.scatter(xc, yc, label=\"Context\", style=\"train\", s=20)\n",
    "    plt.scatter(xt, yt, label=\"Target\", style=\"test\", s=20)\n",
    "\n",
    "    # Plot prediction.\n",
    "    err = 1.96 * scale\n",
    "    plt.plot(xt, mean, label=\"Prediction\", style=\"pred\")\n",
    "    plt.fill_between(xt, mean - err, mean + err, style=\"pred\")\n",
    "\n",
    "    plt.xlim(xt.min(), xt.max())\n",
    "    wbml.plot.tweak()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def ic_posterior_samples(model, n_samples=10, gp_datasets=gp_datasets, n=200, max_n_cntxt=60, max_n_dc=10):\n",
    "    for _ in range(n_samples):\n",
    "        xc, yc, xt, yt, dc = ic_batch_generator(gp_datasets, batch_size=1, n=n, max_n_cntxt=max_n_cntxt, max_n_dc=max_n_dc)\n",
    "\n",
    "        p_yCc, *_ = model(xc, yc, xt, yt, dc)\n",
    "        mean = p_yCc.base_dist.loc.detach().numpy()\n",
    "        scale = p_yCc.base_dist.scale.detach().numpy()\n",
    "\n",
    "        visualise_1d(mean, scale, xc, yc, xt, yt)\n",
    "\n",
    "def cnp_posterior_samples(model, n_samples=10, gp_datasets=gp_datasets, n=200, max_n_cntxt=60, max_n_dc=10):\n",
    "    for _ in range(n_samples):\n",
    "        xc, yc, xt, yt, dc = ic_batch_generator(gp_datasets, batch_size=1, n=n, max_n_cntxt=max_n_cntxt, max_n_dc=max_n_dc)\n",
    "\n",
    "        p_yCc, *_ = model(xc, yc, xt, yt)\n",
    "        mean = p_yCc.base_dist.loc.detach().numpy()\n",
    "        scale = p_yCc.base_dist.scale.detach().numpy()\n",
    "\n",
    "        visualise_1d(mean, scale, xc, yc, xt, yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4f8c8a-683c-4448-af38-95d8333ac7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_posterior_samples(model)\n",
    "# cnp_posterior_samples(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c742e2-d8d9-4ada-bfe7-a97c2c5e2704",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
